<machine MACH="betzy">
  <DESC> BullSequana XH2000 AMD® Epyc™ "Rome" 2.2GHz, 128-way nodes, os is Linux, batch system is SLURM</DESC>
  <OS>LINUX</OS>
  <COMPILERS>intel</COMPILERS>
  <MPILIBS>openmpi</MPILIBS>
  <CIME_OUTPUT_ROOT>/cluster/work/users/$USER/noresm</CIME_OUTPUT_ROOT>
  <DIN_LOC_ROOT>/cluster/shared/noresm/inputdata</DIN_LOC_ROOT>
  <DIN_LOC_ROOT_CLMFORC>/cluster/shared/noresm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
  <DOUT_S_ROOT>/cluster/work/users/$USER/archive/$CASE</DOUT_S_ROOT>
  <BASELINE_ROOT> /cluster/shared/noresm/noresm_baselines</BASELINE_ROOT>
  <CCSM_CPRNC>/cluster/shared/noresm/tools/cprnc-iompi-2023b/bin/cprnc</CCSM_CPRNC>
  <GMAKE_J>8</GMAKE_J>
  <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
  <SUPPORTED_BY>noresmCommunity</SUPPORTED_BY>
  <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
  <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
  <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
  <mpirun mpilib="mpi-serial">
    <executable></executable>
  </mpirun>
  <mpirun mpilib="openmpi">
    <executable>srun --kill-on-bad-exit --label</executable>
  </mpirun>
  <module_system type="module" allow_error="true">
    <init_path lang="perl">$ENV{LMOD_PKG}/init/perl</init_path>
    <init_path lang="python">$ENV{LMOD_PKG}/init/env_modules_python.py</init_path>
    <init_path lang="csh">$ENV{LMOD_PKG}/init/csh</init_path>
    <init_path lang="sh">$ENV{LMOD_PKG}/init/sh</init_path>
    <cmd_path lang="perl">$ENV{LMOD_PKG}/libexec/lmod perl</cmd_path>
    <cmd_path lang="python">$ENV{LMOD_PKG}/libexec/lmod python</cmd_path>
    <cmd_path lang="sh">module</cmd_path>
    <cmd_path lang="csh">module</cmd_path>
    <modules compiler="intel" mpilib='openmpi'>
      <command name="--force purge"></command>
      <command name="load">StdEnv</command>
      <command name="use">/cluster/shared/noresm/eb_mods_2023b/modules/all</command>
      <command name="load">git/2.42.0-GCCcore-13.2.0</command>
      <command name="load">ESMF/8.6.1-iomkl-2023b</command>
      <command name="load">ParMETIS/4.0.3-iompi-2023b</command>
      <command name="load">Python/3.11.5-GCCcore-13.2.0</command>
      <command name="load">CMake/3.27.6-GCCcore-13.2.0</command>
      <command name="load">XML-LibXML/2.0210-GCCcore-13.2.0</command>
    </modules>
  </module_system>
  <environment_variables>
    <env name="ESMFMKFILE">$ENV{EBROOTESMF}/lib/esmf.mk</env>
    <env name="ESMF_RUNTIME_PROFILE">ON</env>
    <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
    <env name="HCOLL_MAIN_IB">mlx5_0:1</env>
    <env name="KMP_STACKSIZE">64M</env>
    <env name="MKL_DEBUG_CPU_TYPE">5</env>
    <env name="PIO_TYPENAME_VALID_VALUES">pnetcdf,netcdf,netcdf4p,netcdf4c</env>
    <env name="OMPI_MCA_mpi_leave_pinned">1</env>
    <env name="OMPI_MCA_btl">self,vader</env>
    <env name="OMPI_MCA_rmaps_rank_file_physical">1</env>
    <env name="OMPI_MCA_coll_hcoll_enable">1</env>
    <env name="OMPI_MCA_coll">^fca</env>
    <env name="OMPI_MCA_coll_hcoll_priority">95</env>
    <env name="OMPI_MCA_coll_hcoll_np">8</env>
    <env name="OMPI_MCA_io">ompio</env>
    <env name="OMPI_MCA_fs_lustre_stripe_size">1048576</env>
    <env name="OMPI_MCA_fs_lustre_stripe_width">8</env>
    <env name="OMPI_MCA_sharedfp">^lockedfile,individual</env>
    <env name="I_MPI_EXTRA_FILESYSTEM_LIST">lustre</env>
    <env name="I_MPI_EXTRA_FILESYSTEM">on</env>
    <!--env name="OMPI_MCA_fs_lustre_stripe_size">1048576</env-->
    <!--env name="OMPI_MCA_fs_lustre_stripe_width">8</env-->
  </environment_variables>
  <resource_limits>
    <resource name="RLIMIT_STACK">-1</resource>
  </resource_limits>
</machine>